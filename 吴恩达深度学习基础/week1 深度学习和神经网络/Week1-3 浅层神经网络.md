# 1. 浅层神经网络

## 1.1. 神经网络概述

逻辑回归模型

![](images/2019-12-22-10-49-16.png)

公式3.1

![](images/2019-12-22-10-53-09.png)

**将逻辑回归模型与公式3.1联系起来**

首先，需要输入特征x，参数$\omega$和b，然后计算出z

![](images/2019-12-22-10-54-34.png)

利用z可以计算出$\alpha$，我们的符号表示输出$\hat{y}=>\alpha=\sigma(z)$,，然后利用$\alpha$，可以计算loss function $L(\alpha/\hat{y},y)$

**将多个sigmoid单元堆叠起来形成一个神经网络**

![](images/2019-12-22-10-58-32.png)

在这个神经网络中，首先计算第一层网络中的各个节点相关的数$z^{[1]}$，接着计算$\alpha^{[1]}$，在计算下一层网络同理，符号$^{[m]}$表示第m层网络中节点相关的数，这些节点的集合被称为第m层网络。

*计算过程*
***

![](images/2019-12-22-11-06-43.png)

![](images/2019-12-22-11-06-53.png)

类似逻辑回归，在计算后需要使用计算，接下来你要使用另一个线性方程对应的参数计算$z^{[2]}$和$\alpha^{[2]}$，此时$\alpha^{[2]}$就是整个神经网络最终的输出。

*疑问*
即在原来的逻辑回归中，只是通过直接计算z得到结果$\alpha$。而这个神经网络中，我们反复的计算z和$\alpha$，计算$\alpha$和z，最后得到了最终的输出loss function。

![](images/2019-12-22-11-11-59.png)

在神经网络中，也要像上图这样从右往左反向计算导数。

## 1.2. 神经网络的表示(Representation)

*本例的神经网络只有一个`隐藏层`*

![](images/2019-12-22-11-20-14.png)

+ 输入层

    输入特征$x_1,x_2,x_3$竖直堆叠形成，包含了神经网络的输入
+ 隐藏层

    在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入x也包含了目标输出y，隐藏层是在训练集中无法看到的值。
+ 输出层

    负责产生预测值

之前用向量x表示输出特征，这里用$a^{[0]}$来代替，即输入层的激活值是$a^{[0]}$，隐藏层的激活值记做$a^{[1]}$，则其中第i个单元的值为$a_i^{[1]}$，此处我们的隐藏层有4个单元，则可写为

![](images/2019-12-22-13-00-01.png)

最后输出层将产生某个数值a，所以$\hat{y}$值将取为$a^{[2]}$。在逻辑回归中只有一个输出层，所以没有使用带方括号的上标。

*但是在神经网络中，我们使用这种带上标的形式来明确地指出这些值来自于哪一层*

![](images/2019-12-22-13-05-47.png)

上图只能叫做一个两层的神经网络，原因是当我们计算网络的层数时，输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。第二个惯例是我们将输入层称为第零层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出层。

但是在传统的符号使用中，如果你阅读研究论文或者在这门课中，你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。

**隐藏层和输出层的参数**
+ 该例中隐藏层有2个传输W和b，那么$W^{[1]},b^{[1]}$代表它是来自于隐藏层的参数。

    $W^{[1]}$是一个4*3的矩阵，而$b^{[1]}$是一个4*1的向量。
    + 4源自于我们有4个隐藏层单元
    + 3源自这里有3个输入特征

+ 该例中输出层同样也有参数$W^{[2]}$和$b^{[2]}$。

    $W^{[2]}$是一个1*4的矩阵，而$b^{[2]}$是一个1*1的向量。
    + 1*4是因为隐藏层有4个隐藏层单元和1个输出层单元
    + 1*1是因为有1个输出层单元

## 1.3. 计算一个神经网络的输出

![](images/2019-12-22-13-31-41.png)

每个圆圈就是神经网络的计算单元，分为2步
+ 按步骤计算出z
+ 以sigmoid函数为激活函数计算z得到a

每个神经网络只是这样做了很多次重复计算。

隐藏层的隐藏单元计算如下:
+ $z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},a_1^{[1]}=\sigma(z_1^{[1]})$
+ $z_2^{[2]}=w_2^{[2]T}x+b_2^{[2]},a_2^{[2]}=\sigma(z_2^{[2]})$
+ $z_3^{[3]}=w_3^{[3]T}x+b_3^{[3]},a_3^{[3]}=\sigma(z_3^{[3]})$
+ $z_4^{[4]}=w_4^{[4]T}x+b_4^{[4]},a_4^{[4]}=\sigma(z_4^{[4]})$

**向量化技术**

向量化的过程是将神经网络中的一层神经元参数纵向堆积起来形成一个4*3的矩阵，用符号$W^{[1]}$表示。

上面4行就改写成了 $z^{[n]}=w^{[n]}x+b^{[n]},a^{[n]}=\sigma(z^{[n]})$

![](images/2019-12-22-14-01-17.png)

![](images/2019-12-22-14-02-40.png)

同理，输出层同样可以写成类似的形式，得到$a^{[2]}=\hat{y}$

**总结**
***

![](images/2019-12-22-14-05-44.png)

这一节所讲内容包括公式和向量化过程对比上一章的逻辑回归只是多了一个隐藏层，它全部的实现过程都是上图那4行（向量化），编程也是只需要实现这4行就可以前向传播计算出预测值。

## 1.4. 前向传播的多样本向量化

多个训练样本在二层神经网络上的向量化。

则上述所讲的单个训练样本的过程变成了此时的其中1环

*第i个样本*
***

![](images/2019-12-22-14-12-26.png)

**向量化**

*输入的x变成了m列，那么输出的a也针对性地变成了m列，每列代表每个样本的输出，b会使用python广播机制扩充*

*相当于原来一组节点的参数只在一组样本上历练，现在开始在m组上历练有了m个输出值*

+ 输入的样本训练集

    ![](images/2019-12-22-14-14-17.png)

+ 隐藏层第1步计算的输出值

    ![](images/2019-12-22-14-15-07.png)

+ 隐藏层第2步计算的输出值

    ![](images/2019-12-22-14-15-25.png)

总结起来就是，左侧的未向量化公式转化为了右侧的向量化表达。

![](images/2019-12-22-14-15-56.png)

*不同的样本是水平排列的，不同的参数(神经网络结点)是垂直排列的*

*每组样本有多组参数利用，每组参数也有多组样本检验*

## 1.5. 激活函数

*非线性函数*

+ tanh函数

    $$tanhx=\frac{sinhx}{coshx}=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$

    ![](images/2019-12-22-14-37-06.png)

    *值域位于-1和1之间而且穿过了原点，可以理解是sigmod的平移和伸缩*

    **结果表明，在隐藏层上使用tanh函数效果总是优于sigmoid函数**,因为函数值在-1和+1间的激活函数，其均值更接近0。

    在讨论优化算法时，有一点要说明：我基本已经不用sigmoid激活函数了，tanh函数在所有场合都优于sigmoid函数。但在儿分类问题中，由于结果必须处于0-1间的限制，需要使用sigmoid函数，但我们**可以在隐藏层使用tanh函数，而在输出层使用sigmod函数**

    *缺点*

    sigmoid和tanh函数共同缺点是在输入值特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度

+ Relu函数

    $$f(x)=max(0,x)$$

    ![](images/2019-12-22-14-46-12.png)

    *只要输入是正值时导数恒为1，负值时恒为0*

    *但该函数在x=0处不可导，但编程时我们会令x=0.00000001，并假设其导数是0或1都可以*

**选择激活函数的法则**

+ **在二分类问题中**输出层选sigmoid函数，其他的所有单元都选择Relu函数。
+ 如果隐藏层不确定使用哪个函数，通常使用Relu函数，有时也使用tanh函数

+ Leaky Relu函数

    *当输入是负值时，这个函数的值不等于0，而是轻微的倾斜*

    ![](images/2019-12-22-15-00-39.png)

两者的优点是：

第一，在输入值的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，Relu在程序实现就是一个if-else语句，而sigmoid函数需要进行浮点四则运算，在实践中，使用ReLu激活函数神经网络通常会比使用sigmoid或者tanh激活函数学习的更快。

第二，sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而Relu和Leaky ReLu函数大于0部分都为常数，不会产生`梯度弥散`现象。(同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而Leaky ReLu不会有这问题)

*梯度弥散*
***
在反向传播过程中梯度越来越小，即越靠近输入层的隐藏层梯度小，参数更新慢。

**总结**
***

+ sigmoid函数：除了输出层是二分类问题就奔不会使用
+ tanh函数：非常优秀，几乎适用于所有场合
+ Relu函数：最常用的默认函数，如果不确认用哪个就使用Relu或者Leaky Relu

通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。

## 1.6. 为什么需要非线性激活函数

总而言之，不能在隐藏层用线性激活函数，可以用ReLU或者tanh或者leaky ReLU或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。在这之外，在隐层使用线性激活函数非常少见。因为房价都是非负数，所以我们也可以在输出层使用ReLU函数这样你的都大于等于0。

首先明确一点，**激活函数是用来加入非线性因素的，因为线性模型的表达力不够**。
假设如果没有激活函数的出现，你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，也就是说没有激活函数的每层都相当于矩阵相乘。就算你叠加了若干层之后，无非还是个矩阵相乘罢了。那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激活函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可逼近任意函数）

不使用激活函数的简单逻辑回归

![](images/2019-12-22-15-11-43.png)

使用激活函数

![](images/2019-12-22-15-12-03.png)

可见，激活函数能帮助我们引入非线性因素，使得神经网络能够更好地解决更加复杂的问题。

## 1.7. 神经网络的梯度下降

在训练神经网络的时候，随机初始化参数很重要，而不是初始化成全零。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值。

$dW^{[1]}=\frac{dJ}{dW[1]},db^{[1]}=\frac{dJ}{db^{[1]}}$
$dW^{[2]}=\frac{dJ}{dW[2]},db^{[2]}=\frac{dJ}{db^{[2]}}$

然后,

$W^{[1]}:=W^{[1]}-\alpha*dW^{[1]},b^{[1]}:=b^{[1]}-\alpha*db^{[1]}$
$W^{[2]}:=W^{[2]}-\alpha*dW^{[2]},b^{[2]}:=b^{[2]}-\alpha*db^{[2]}$

根据上张知识，我们很容易写出神经网络反向传播的向量化过程表达如下：

+ 输出层
  
    $dz^{[2]}=A^{[2]}-Y,Y=[y^{[1]},...]$
    $dW^{[2]}=\frac{1}{m}dz^{[2]}A^{[1]T}$
    $db^{[2]}=\frac{1}{m}np.sum(dz^{2},axis=1,keepdims=True)$
+ 隐藏层
  
    ![](images/2019-12-22-15-39-40.png)

    $dW^{[1]}=\frac{1}{m}dz^{[1]}A^{[1]T}$
    $db^{[1]}=\frac{1}{m}np.sum(dz^{1},axis=1,keepdims=True)$

这里np.sum是python的numpy命令，axis=1表示水平相加求和，keepdims是防止python输出那些古怪的秩数，加上这个确保阵矩阵$db^{[2]}$这个向量输出的维度为(n,1)这样标准的形式。

## 1.8. 理解反向传播

反向传播公式的具体推到过程。

时间原因未学习。

[地址](http://www.ai-start.com/dl2017/html/lesson1-week3.html)

## 1.9. 随机初始化

对于逻辑回归，把权重初始化为0当然也是可以的。

但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降将不会起作用。

如果你要初始化成0，由于所有的隐含单元都是对称的，无论你运行梯度下降多久，他们一直计算同样的函数。

*这没有任何帮助，因为你想要`两个不同的隐含单元计算不同的函数`*

*解决方案*
***

**随机初始化参数**

把$W^{[1]}$设为np.random.randn($n^{[1]},n^{[0]}$)(生成高斯分布),通常再乘上一个小的数，比如`0.01`，这样把它初始化为很小的随机数。

b则可以初始化为0，因为不会出现上述问题

*$W^{[2]}和b^{[2]}$的初始化方式同理*

![](images/2019-12-22-15-49-44.png)

**乘以0.01的原因**

由于输入值太大对于sigmod函数或者tanh函数会造成梯度弥散，即梯度下降太慢，造成学习也很慢。但如果没有这两种函数就不成问题。

事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能要试试0.01以外的常数。