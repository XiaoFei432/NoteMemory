# 深层神经网络

## 深层神经网络

有一些函数，只有非常深的神经网络能学会，而更浅的模型则办不到。

尽管对于任何给定的问题很难去提前预测到底需要多深的神经网络，所以先去尝试逻辑回归，尝试一层然后两层隐含层，然后把隐含层的数量看做是另一个可以自由选择大小的超参数，然后再保留交叉验证数据上评估，或者用你的开发集来评估。

总结下符号约定

输入的特征记做x，但是x同样也是0层的激活函数，所以$x=a^{[0]}$，$a^{[L]}$是这个神经网络预测的输出。

## 前向传播

输入是$a^{[l-1]}$，输出是$a^{[l]}$,缓存是$z^{[l]}$(从实现的角度看缓存的是$w^{[L]}和b^{[L]}$)

*缓存*
把反向函数计算出来的值缓存下来，缓存很方便，可以迅速得到$w^{[l]}$和$b^{[l]}$的值。

**步骤**

$z^{[l]}=W^{[l]}·a^{[l-1]}+b^{[l]}$

$a^{[l]}=g^{[l]}(z^{[l]})$

## 反向传播

输入为$da^{[l]}$,输出为$da^{[l-1]}$,$dw^{[l]},db^{[l]}$

**步骤**

1. $d{{z}^{[l]}}=d{{a}^{[l]}}*{{g}^{[l]}}'( {{z}^{[l]}}) \quad \quad$    //a对z求导，先整体求导，然后再对函数g求导

2. $d{{w}^{[l]}}=d{{z}^{[l]}}\cdot{{a}^{[l-1]}}~\quad \quad$  //a对w求导，要先对z整体求导，然后z对w求导

3. $d{{b}^{[l]}}=d{{z}^{[l]}}~~\quad \quad$ //  a对w求导，要先对z整体求导，然后z对w求导=1

4. $d{{a}^{[l-1]}}={{w}^{\left[ l \right]T}}\cdot {{dz}^{[l]}}\quad\quad$ //  a对输入a-1求导，先对z求导，然后z在对a-1求导

5. $d{{z}^{[l]}}={{w}^{[l+1]T}}d{{z}^{[l+1]}}\cdot \text{ }{{g}^{[l]}}'( {{z}^{[l]}})~\quad \quad$  // 代入得到对缓存z的求导  

**向量化**

1. $d{{Z}^{[l]}}=d{{A}^{[l]}}*{{g}^{\left[ l \right]}}'\left({{Z}^{[l]}} \right)~~$

2. $d{{W}^{[l]}}=\frac{1}{m}\text{}d{{Z}^{[l]}}\cdot {{A}^{\left[ l-1 \right]T}}$

3. $d{{b}^{[l]}}=\frac{1}{m}\text{ }np.sum(d{{z}^{[l]}},axis=1,keepdims=True)$

4. $d{{A}^{[l-1]}}={{W}^{\left[ l \right]T}}.d{{Z}^{[l]}}$

5. $dZ^{[l]}=W^{[l+1]T}·dZ^{[l+1]}*{g^{[l]}}^{\prime}(Z^{[l]})$

![](images/2020-01-04-15-30-01.png)

第一层你可能有一个ReLU激活函数，第二层为另一个ReLU激活函数，第三层可能是sigmoid函数（如果你做二分类的话）。这样你就可以向后迭代进行反向传播来求导来求$dw^{[3]},db^{[3]},dw^{[2]},db^{[2]},dw^{[1]},db^{[1]}$。在计算的时候，缓存会把$z^{[1]},z^{[2]},z^{[3]}$传递过来，然后回传$da^{[2]},da^{[1]}$，可以用来计算$da^{[0]}$，但我们不会使用它，这里讲述了一个三层网络的前向和反向传播。

## 深层网络中的前向传播

迭代步骤

$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$

$a^{[l]}=g^{[l]}(z^{[l]})$

向量化

$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$

$A^{[l]}=g^{[l]}(Z^{[l]})\quad(A^{[0]}=X)$

## 核对矩阵的维数

$w^{[l]}:(n^{[l]},n^{[l-1]})$ (当前层的节点数//对应有多少行权值，和上一层的节点数//对应本列的输入有多少个特征)

若有m个样本，则$A^{[0]}=X(n_x,m)$,可推出$A^{[l]}(n^{[l]},m)$($n^{[l]}$对应每个样本再上一层的输出值（结点）个数，组成新的特征进行迭代，m代表m个样本)

## 为什么用深层表示？

**深度网络到底在计算什么？**

![](images/2020-01-04-17-10-21.png)

如果你在建一个人脸识别或是人脸检测系统，深度神经网络所做的事就是，当你输入一张脸部的照片，然后你可以把深度神经网络的第一层，当成一个特征探测器或者边缘探测器。在这个例子里，我会建一个大概有20个隐藏单元的深度神经网络，是怎么针对这张图计算的。隐藏单元就是这些图里这些小方块（第一张大图），举个例子，这个小方块（第一行第一列）就是一个隐藏单元，它会去找这张照片里“|”边缘的方向。那么这个隐藏单元（第四行第四列），可能是在找（“—”）水平向的边缘在哪里。之后的课程里，我们会讲专门做这种识别的卷积神经网络，到时候会细讲，为什么小单元是这么表示的。你可以先把神经网络的第一层当作看图，然后去找这张照片的各个边缘。我们可以把照片里组成边缘的像素们放在一起看，然后它可以把被探测到的边缘组合成面部的不同部分（第二张大图）。比如说，可能有一个神经元会去找眼睛的部分，另外还有别的在找鼻子的部分，然后把这许多的边缘结合在一起，就可以开始检测人脸的不同部分。最后再把这些部分放在一起，比如鼻子眼睛下巴，就可以识别或是探测不同的人脸（第三张大图）。

所以深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。比如你录在音频里的单词、词组或是句子，然后就能运行语音识别了。同时我们所计算的之前的几层，也就是相对简单的输入函数，比如图像单元的边缘什么的。到网络中的深层时，你实际上就能做很多复杂的事，比如探测面部或是探测单词、短语或是句子。

有些人喜欢把深度神经网络和人类大脑做类比，这些神经科学家觉得人的大脑也是先探测简单的东西，比如你眼睛看得到的边缘，然后组合起来才能探测复杂的物体，比如脸。这种深度学习和人类大脑的比较，有时候比较危险。但是不可否认的是，我们对大脑运作机制的认识很有价值，有可能大脑就是先从简单的东西，比如边缘着手，再组合成一个完整的复杂物体，这类简单到复杂的过程，同样也是其他一些深度学习的灵感来源，之后的视频我们也会继续聊聊人类或是生物学理解的大脑

**吴恩达的深度学习过程**

但是当我开始解决一个新问题时，我通常会从logistic回归开始，再试试一到两个隐层，把隐藏层数量当作参数、超参数一样去调试，这样去找比较合适的深度。但是近几年以来，有一些人会趋向于使用非常非常深邃的神经网络，比如好几打的层数，某些问题中只有这种网络才是最佳模型。

## 搭建神经网络块

![](images/2020-01-04-17-18-54.png)

把输入特征$a^{[0]}$，放入第一层并计算第一层的激活函数，用$a^{[1]}$表示，你需要$W^{[1]}$和$b^{[1]}$来计算，之后也缓存$z^{[l]}$值。之后喂到第二层，第二层里需要用到$W^{[2]}$和$b^{[2]}$，你会需要计算第二层的激活函数$a^{[2]}$。后面几层以此类推，直到最后你算出了$a^{[L]}$，第$L$层的最终输出值$\hat y$。在这些过程里我们缓存了所有的$z$值，这就是正向传播的步骤。

![](images/2020-01-04-17-19-02.png)

对反向传播的步骤而言，我们需要算一系列的反向迭代，就是这样反向计算梯度，你需要把$da^{[L]}$的值放在这里，然后这个方块会给我们${da}^{[L-1]}$的值，以此类推，直到我们得到${da}^{[2]}$和${da}^{[1]}$，你还可以计算多一个输出值，就是${da}^{[0]}$，但这其实是你的输入特征的导数，并不重要，起码对于训练监督学习的权重不算重要，你可以止步于此。反向传播步骤中也会输出$dW^{[l]}$和$db^{[l]}$，这会输出$dW^{[3]}$和$db^{[3]}$等等。目前为止你算好了所有需要的导数，稍微填一下这个流程图。

神经网络的一步训练包含了，从$a^{[0]}$开始，也就是 $x$ 然后经过一系列正向传播计算得到$\hat y$，之后再用输出值计算这个（第二行最后方块），再实现反向传播。现在你就有所有的导数项了，$W$也会在每一层被更新为$W=W-αdW$，$b$也一样，$b=b-αdb$，反向传播就都计算完毕，我们有所有的导数值，那么这是神经网络一个梯度下降循环。

## 参数vs超参数

**什么是超参数？（区别于特征的权，模型执行前必备的参数）**

比如算法中的learning rate $a$（学习率）、iterations(梯度下降法循环的数量)、$L$（隐藏层数目）、${{n}^{[l]}}$（隐藏层单元数目）、choice of activation function（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数$W$和$b$的值，所以它们被称作超参数。

**如何寻找超参数的最优值？**

![](images/2020-01-04-17-24-25.png)

走Idea—Code—Experiment—Idea这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代。
